<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lesson 9 - Training Deep Neural Networks</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #333;
    }

    header {
      background-color: #2e7d32;
      color: white;
      padding: 20px;
      text-align: center;
    }

    main {
      padding: 20px;
      max-width: 900px;
      margin: auto;
      background: white;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
      border-radius: 8px;
    }

    h1 {
      color: #2e7d32;
    }

    h2 {
      margin-top: 30px;
      color: #444;
    }

    p {
      line-height: 1.7;
      margin-bottom: 15px;
    }

    ul {
      margin: 10px 0 20px 20px;
    }

    pre {
      background: #eee;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
    }

    footer {
      background-color: #333;
      color: white;
      text-align: center;
      padding: 20px;
      margin-top: 40px;
    }

    .footer-ad {
      margin-bottom: 10px;
    }

    .nav-links {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
    }

    .nav-links a {
      color: #2e7d32;
      text-decoration: none;
      font-weight: bold;
    }

    .nav-links a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <header>
    <h1>Lesson 9: Training Deep Neural Networks</h1>
  </header>

  <main>
    <h2>Overview</h2>
    <p>
      As neural networks grow deeper, training becomes more challenging. This lesson covers techniques to improve performance and training time for deep neural networks (DNNs).
    </p>

    <h2>Challenges in Deep Networks</h2>
    <ul>
      <li>Vanishing/Exploding gradients</li>
      <li>Overfitting due to too many parameters</li>
      <li>High computation time</li>
    </ul>

    <h2>Techniques to Improve Training</h2>
    <ul>
      <li><strong>Batch Normalization</strong> – Normalizes inputs in hidden layers to speed up training.</li>
      <li><strong>Dropout</strong> – Randomly drops neurons during training to prevent overfitting.</li>
      <li><strong>Gradient Clipping</strong> – Prevents gradients from becoming too large.</li>
      <li><strong>Adaptive Optimizers</strong> – Use optimizers like Adam, RMSprop instead of basic SGD.</li>
    </ul>

    <h2>Sample with Keras</h2>
    <pre><code>
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])
    </code></pre>

    <h2>Conclusion</h2>
    <p>
      DNNs are powerful, but need careful design and tuning. With the right practices, you can train deep models that generalize well and perform efficiently.
    </p>

    <div class="nav-links">
      <a href="lesson8.html">&larr; Back to Lesson 8</a>
      <a href="lesson10.html">Next Lesson &rarr;</a>
    </div>
  </main>

  <footer>
    <div class="footer-ad">
      <!-- Your Ad Code Goes Here -->
      <p>Advertisement Space</p>
    </div>
    <p>&copy; 2025 YourSiteName. All rights reserved.</p>
  </footer>

</body>
  </html>
