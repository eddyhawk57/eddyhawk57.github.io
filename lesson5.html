<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lesson 5 - Evaluating Machine Learning Models</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #333;
    }

    header {
      background-color: #2e7d32;
      color: white;
      padding: 20px;
      text-align: center;
    }

    main {
      padding: 20px;
      max-width: 900px;
      margin: auto;
      background: white;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
      border-radius: 8px;
    }

    h1 {
      color: #2e7d32;
    }

    h2 {
      margin-top: 30px;
      color: #444;
    }

    p {
      line-height: 1.7;
      margin-bottom: 15px;
    }

    ul {
      margin: 10px 0 20px 20px;
    }

    pre {
      background: #eee;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
    }

    footer {
      background-color: #333;
      color: white;
      text-align: center;
      padding: 20px;
      margin-top: 40px;
    }

    .footer-ad {
      margin-bottom: 10px;
    }

    .nav-links {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
    }

    .nav-links a {
      color: #2e7d32;
      text-decoration: none;
      font-weight: bold;
    }

    .nav-links a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <header>
    <h1>Lesson 5: Evaluating Machine Learning Models</h1>
  </header>

  <main>
    <h2>Why Evaluation Matters</h2>
    <p>
      Once you've trained a machine learning model, it's crucial to assess its performance. Evaluation helps you understand how well the model generalizes to new, unseen data and guides further improvements.
    </p>

    <h2>Common Evaluation Metrics</h2>
    <ul>
      <li><strong>Accuracy:</strong> Percentage of correct predictions.</li>
      <li><strong>Precision:</strong> Of all predicted positives, how many were correct?</li>
      <li><strong>Recall:</strong> Of all actual positives, how many were found?</li>
      <li><strong>F1-Score:</strong> Harmonic mean of precision and recall.</li>
      <li><strong>Confusion Matrix:</strong> Table that shows true vs predicted values.</li>
    </ul>

    <h2>Python Example</h2>
    <pre><code>
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Assume you have y_test and predictions from previous lesson
print("Accuracy:", accuracy_score(y_test, predictions))
print("Precision:", precision_score(y_test, predictions))
print("Recall:", recall_score(y_test, predictions))
print("F1 Score:", f1_score(y_test, predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, predictions))
    </code></pre>

    <h2>Visualizing Performance</h2>
    <p>
      Tools like <strong>Seaborn</strong> and <strong>Matplotlib</strong> can help you visualize confusion matrices and classification reports, making it easier to spot model weaknesses.
    </p>

    <h2>Conclusion</h2>
    <p>
      Evaluation is not just about numbers â€” it's about understanding the strengths and weaknesses of your model. The next lesson will explore how to improve performance with tuning and cross-validation.
    </p>

    <div class="nav-links">
      <a href="lesson4.html">&larr; Back to Lesson 4</a>
      <a href="lesson6.html">Next Lesson &rarr;</a>
    </div>
  </main>

  <footer>
    <div class="footer-ad">
      <!-- Your Ad Code Goes Here -->
      <p>Advertisement Space</p>
    </div>
    <p>&copy; 2025 YourSiteName. All rights reserved.</p>
  </footer>

</body>
  </html>
