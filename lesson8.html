<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lesson 8 - Building a Neural Network from Scratch</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #333;
    }

    header {
      background-color: #2e7d32;
      color: white;
      padding: 20px;
      text-align: center;
    }

    main {
      padding: 20px;
      max-width: 900px;
      margin: auto;
      background: white;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
      border-radius: 8px;
    }

    h1 {
      color: #2e7d32;
    }

    h2 {
      margin-top: 30px;
      color: #444;
    }

    p {
      line-height: 1.7;
      margin-bottom: 15px;
    }

    ul {
      margin: 10px 0 20px 20px;
    }

    pre {
      background: #eee;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
    }

    footer {
      background-color: #333;
      color: white;
      text-align: center;
      padding: 20px;
      margin-top: 40px;
    }

    .footer-ad {
      margin-bottom: 10px;
    }

    .nav-links {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
    }

    .nav-links a {
      color: #2e7d32;
      text-decoration: none;
      font-weight: bold;
    }

    .nav-links a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <header>
    <h1>Lesson 8: Building a Neural Network from Scratch</h1>
  </header>

  <main>
    <h2>Objective</h2>
    <p>
      In this lesson, you'll learn how to implement a simple feedforward neural network from scratch using Python and NumPy. This will help you understand how each part of the network functions under the hood.
    </p>

    <h2>Step-by-Step Code</h2>
    <pre><code>
import numpy as np

# Sigmoid activation
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative for backpropagation
def sigmoid_derivative(x):
    return x * (1 - x)

# Input data (4 samples, 3 features)
X = np.array([
    [0, 0, 1],
    [1, 1, 1],
    [1, 0, 1],
    [0, 1, 1]
])

# Output data
y = np.array([[0], [1], [1], [0]])

# Seed for consistent results
np.random.seed(1)

# Random weights (3 inputs -> 1 output)
weights = 2 * np.random.random((3, 1)) - 1

# Training loop
for i in range(10000):
    input_layer = X
    outputs = sigmoid(np.dot(input_layer, weights))
    error = y - outputs
    adjustments = error * sigmoid_derivative(outputs)
    weights += np.dot(input_layer.T, adjustments)

print("Trained outputs:")
print(outputs)
    </code></pre>

    <h2>Whatâ€™s Happening Here</h2>
    <ul>
      <li>Random weights are assigned initially</li>
      <li>We compute outputs using a sigmoid function</li>
      <li>Error is calculated and backpropagated to adjust weights</li>
      <li>We iterate this process to "learn" the correct weights</li>
    </ul>

    <h2>Conclusion</h2>
    <p>
      This is a basic neural net with one layer. In the next lessons, you'll learn how to expand it with multiple layers and advanced optimizations using frameworks like TensorFlow and PyTorch.
    </p>

    <div class="nav-links">
      <a href="lesson7.html">&larr; Back to Lesson 7</a>
      <a href="lesson9.html">Next Lesson &rarr;</a>
    </div>
  </main>

  <footer>
    <div class="footer-ad">
      <!-- Your Ad Code Goes Here -->
      <p>Advertisement Space</p>
    </div>
    <p>&copy; 2025 YourSiteName. All rights reserved.</p>
  </footer>

</body>
</html>
