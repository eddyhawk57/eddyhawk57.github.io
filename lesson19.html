<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Lesson 19 - Long Short-Term Memory (LSTM) Networks</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #333;
    }

    header {
      background-color: #2e7d32;
      color: white;
      padding: 20px;
      text-align: center;
    }

    main {
      padding: 20px;
      max-width: 900px;
      margin: auto;
      background: white;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
      border-radius: 8px;
    }

    h1 {
      color: #2e7d32;
    }

    h2 {
      margin-top: 30px;
      color: #444;
    }

    p {
      line-height: 1.7;
      margin-bottom: 15px;
    }

    ul {
      margin: 10px 0 20px 20px;
    }

    pre {
      background: #eee;
      padding: 10px;
      border-radius: 5px;
      overflow-x: auto;
    }

    footer {
      background-color: #333;
      color: white;
      text-align: center;
      padding: 20px;
      margin-top: 40px;
    }

    .footer-ad {
      margin-bottom: 10px;
    }

    .nav-links {
      display: flex;
      justify-content: space-between;
      margin-top: 30px;
    }

    .nav-links a {
      color: #2e7d32;
      text-decoration: none;
      font-weight: bold;
    }

    .nav-links a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <header>
    <h1>Lesson 19: Long Short-Term Memory (LSTM) Networks</h1>
  </header>

  <main>
    <h2>Introduction</h2>
    <p>
      Long Short-Term Memory (LSTM) networks are a special kind of RNN capable of learning long-term dependencies. They are designed to remember information for long periods and avoid the vanishing gradient problem common in traditional RNNs.
    </p>

    <h2>Why LSTMs Were Developed</h2>
    <p>
      Traditional RNNs struggle with long sequences due to their inability to preserve information across time steps. LSTMs solve this by using a memory cell and gating mechanisms to control the flow of information.
    </p>

    <h2>LSTM Key Components</h2>
    <ul>
      <li><strong>Forget Gate</strong> – Decides what to discard from memory</li>
      <li><strong>Input Gate</strong> – Decides what new information to add</li>
      <li><strong>Output Gate</strong> – Controls what part of memory is output</li>
    </ul>

    <h2>Basic LSTM Code Example (Keras)</h2>
    <pre><code>
from keras.models import Sequential
from keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(50, activation='tanh', input_shape=(10, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()
    </code></pre>

    <h2>Conclusion</h2>
    <p>
      LSTMs are widely used in tasks like text generation, machine translation, and stock forecasting. Their ability to retain information over long sequences makes them invaluable in deep learning workflows involving time-dependent data.
    </p>

    <div class="nav-links">
      <a href="lesson18.html">&larr; Back to Lesson 18</a>
      <a href="lesson20.html">Next Lesson &rarr;</a>
    </div>
  </main>

  <footer>
    <div class="footer-ad">
      <!-- Your Ad Code Goes Here -->
      <p>Advertisement Space</p>
    </div>
    <p>&copy; 2025 YourSiteName. All rights reserved.</p>
  </footer>

</body>
</html>
